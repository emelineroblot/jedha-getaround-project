PROJECT - Deployment
Getaround Analysis üöó
840 min
snap

GetAround is the Airbnb for cars. You can rent cars from any person for a few hours to a few days! Founded in 2009, this company has known rapid growth. In 2019, they count over 5 million users and about 20K available cars worldwide.

As Jedha's partner, they offered this great challenges:

Context
When renting a car, our users have to complete a checkin flow at the beginning of the rental and a checkout flow at the end of the rental in order to:

Assess the state of the car and notify other parties of pre-existing damages or damages that occurred during the rental.
Compare fuel levels.
Measure how many kilometers were driven.
The checkin and checkout of our rentals can be done with three distinct flows:

üì± Mobile rental agreement on native apps: driver and owner meet and both sign the rental agreement on the owner‚Äôs smartphone
Connect: the driver doesn‚Äôt meet the owner and opens the car with his smartphone
üìù Paper contract (negligible)
Project üöß
For this case study, we suggest that you put yourselves in our shoes, and run an analysis we made back in 2017 üîÆ ü™Ñ

When using Getaround, drivers book cars for a specific time period, from an hour to a few days long. They are supposed to bring back the car on time, but it happens from time to time that drivers are late for the checkout.

Late returns at checkout can generate high friction for the next driver if the car was supposed to be rented again on the same day : Customer service often reports users unsatisfied because they had to wait for the car to come back from the previous rental or users that even had to cancel their rental because the car wasn‚Äôt returned on time.

Goals üéØ
In order to mitigate those issues we‚Äôve decided to implement a minimum delay between two rentals. A car won‚Äôt be displayed in the search results if the requested checkin or checkout times are too close from an already booked rental.

It solves the late checkout issue but also potentially hurts Getaround/owners revenues: we need to find the right trade off.

Our Product Manager still needs to decide:

threshold: how long should the minimum delay be?
scope: should we enable the feature for all cars?, only Connect cars?
In order to help them make the right decision, they are asking you for some data insights. Here are the first analyses they could think of, to kickstart the discussion. Don‚Äôt hesitate to perform additional analysis that you find relevant.

Which share of our owner‚Äôs revenue would potentially be affected by the feature?
How many rentals would be affected by the feature depending on the threshold and scope we choose?
How often are drivers late for the next check-in? How does it impact the next driver?
How many problematic cases will it solve depending on the chosen threshold and scope?
Web dashboard
First build a dashboard that will help the product Management team with the above questions. You can use streamlit or any other technology that you see fit.

Machine Learning - /predict endpoint
In addition to the above question, the Data Science team is working on pricing optimization. They have gathered some data to suggest optimum prices for car owners using Machine Learning.

You should provide at least one endpoint /predict. The full URL would look like something like this: https://your-url.com/predict.

This endpoint accepts POST method with JSON input data and it should return the predictions. We assume inputs will be always well formatted. It means you do not have to manage errors. We leave the error handling as a bonus.

Input example:

{
  "input": [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8], [7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]
}
Copy
The response should be a JSON with one key prediction corresponding to the prediction.

Response example:

{
  "prediction":[6,6]
}
Copy
Documentation page
You need to provide the users with a documentation about your API.

It has to be located at the /docs of your website. If we take the URL example above, it should be located directly at https://your-url.com/docs).

This small documentation should at least include:

An h1 title: the title is up to you.
A description of every endpoints the user can call with the endpoint name, the HTTP method, the required input and the expected output (you can give example).
You are free to add other any other relevant informations and style your HTML as you wish.

Online production
You have to host your API online. We recommend you to use Hugging Face as it is free of charge. But you are free to choose any other hosting provider.

Helpers ü¶Æ
To help you start with this project we provide you with some pieces of advice:

Spend some time understanding data
Don't overlook Data Analysis part, there is a lot of insights to find out.
Data Analysis should take 2 to 5 hours
Machine Learning should take 3 to 6 hours
You are not obligated to use libraries to handle your Machine Learning workflow like mlflow but we definitely advise you to do so.
Share your code
In order to get evaluation, do not forget to share your code on a Github repository. You can create a README.md file with a quick description about this project, how to setup locally and the online URL.

Deliverable üì¨
To complete this project, you should deliver:

A dashboard in production (accessible via a web page for example)
The whole code stored in a Github repository. You will include the repository's URL.
An documented online API on Hugging Face server (or any other provider you choose) containing at least one /predict endpoint that respects the technical description above. We should be able to request the API endpoint /predict using curl:
$ curl -i -H "Content-Type: application/json" -X POST -d '{"input": [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]}' http://your-url/predict
Copy
Or Python:

import requests

response = requests.post("https://your-url/predict", json={
    "input": [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]
})
print(response.json())
Copy
Data
There are two files you need to download:

Delay Analysis üëà Data Analysis
Pricing Optimization üëà Machine Learning
Happy coding! üë©‚Äçüíª

========================

Projet GetAround - Plan d'action complet üöó
Bien s√ªr ! Je vais t'accompagner sur ce projet data science. Voici une approche structur√©e pour r√©ussir.
üìã Vue d'ensemble du projet
Tu as 3 livrables principaux :

Dashboard Streamlit - Analyse des retards et impact du d√©lai minimum
API ML - Endpoint de pr√©diction de prix
Documentation - Page /docs de l'API

üéØ Par o√π commencer ?
√âtape 1 : Configuration de l'environnement (30 min)
python# Cr√©e un dossier projet
# getaround-project/
#   ‚îú‚îÄ‚îÄ data/
#   ‚îú‚îÄ‚îÄ notebooks/
#   ‚îú‚îÄ‚îÄ dashboard/
#   ‚îú‚îÄ‚îÄ api/
#   ‚îî‚îÄ‚îÄ requirements.txt

# Requirements essentiels
"""
pandas
numpy
matplotlib
seaborn
streamlit
scikit-learn
fastapi
uvicorn
requests
"""
√âtape 2 : Analyse exploratoire des donn√©es (2-3h)
Je te conseille de commencer par l√†. Une fois que tu auras t√©l√©charg√© les donn√©es :
pythonimport pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Charge tes donn√©es
df_delays = pd.read_csv('data/delay_analysis.csv')
df_pricing = pd.read_csv('data/pricing_data.csv')

# Premiers r√©flexes
print(df_delays.head())
print(df_delays.info())
print(df_delays.describe())
√âtape 3 : Dashboard Streamlit (3-4h)
Cr√©e un fichier dashboard/app.py :
pythonimport streamlit as st
import pandas as pd
import plotly.express as px

st.title("üöó GetAround Delay Analysis")

# Charge les donn√©es
@st.cache_data
def load_data():
    return pd.read_csv('../data/delay_analysis.csv')

df = load_data()

# Analyses √† faire :
# 1. Part du revenu affect√©
# 2. Nombre de locations impact√©es selon le seuil
# 3. Fr√©quence des retards
# 4. Cas probl√©matiques r√©solus
√âtape 4 : ML Model & API (4-5h)
python# api/main.py
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np

app = FastAPI()

# Charge ton mod√®le entra√Æn√©
model = joblib.load('model.pkl')

class PredictionInput(BaseModel):
    input: list

@app.post("/predict")
def predict(data: PredictionInput):
    predictions = model.predict(data.input)
    return {"prediction": predictions.tolist()}
üöÄ Ordre recommand√© d'ex√©cution

Jour 1-2 : Analyse exploratoire compl√®te
Jour 3 : Dashboard Streamlit
Jour 4 : Mod√®le ML + entra√Ænement
Jour 5 : API FastAPI + documentation
Jour 6 : D√©ploiement (Streamlit Cloud + Hugging Face)

==============

Plan D√©taill√© Complet - Projet GetAround üöó
üìä PHASE 1 : ANALYSE EXPLORATOIRE DES DONN√âES (2-3h)
1.1 Configuration initiale (15 min)
python# Structure du projet
"""
getaround-project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ delay_analysis.csv
‚îÇ   ‚îî‚îÄ‚îÄ pricing_data.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA_delays.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 02_EDA_pricing.ipynb
‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ model.pkl
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
"""

# requirements.txt
"""
pandas==2.0.3
numpy==1.24.3
matplotlib==3.7.2
seaborn==0.12.2
plotly==5.15.0
streamlit==1.25.0
scikit-learn==1.3.0
fastapi==0.101.0
uvicorn==0.23.2
pydantic==2.1.1
joblib==1.3.2
requests==2.31.0
"""
1.2 EDA - Donn√©es Retards (1h30)
pythonimport pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ===== CHARGEMENT =====
df_delays = pd.read_csv('data/delay_analysis.csv')

# ===== EXPLORATION INITIALE =====
print("=" * 50)
print("INFORMATIONS G√âN√âRALES")
print("=" * 50)
print(f"Nombre de lignes : {len(df_delays)}")
print(f"Nombre de colonnes : {len(df_delays.columns)}")
print(f"\nNoms des colonnes :\n{df_delays.columns.tolist()}")

# Aper√ßu des donn√©es
print(df_delays.head(10))

# Types et valeurs manquantes
print(df_delays.info())
print("\nValeurs manquantes :")
print(df_delays.isnull().sum())

# Statistiques descriptives
print(df_delays.describe())

# ===== QUESTIONS CL√âS √Ä EXPLORER =====

# Q1: Quelles colonnes sont essentielles ?
# - rental_id, car_id, checkout_time, state, delay_at_checkout_in_minutes
# - checkin_type (Mobile, Connect, Paper)
# - time_delta_with_previous_rental_in_minutes

# Q2: Distribution des retards
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(df_delays['delay_at_checkout_in_minutes'].dropna(), bins=50, edgecolor='black')
plt.xlabel('Retard (minutes)')
plt.ylabel('Fr√©quence')
plt.title('Distribution des retards au checkout')
plt.axvline(x=0, color='red', linestyle='--', label='√Ä l\'heure')
plt.legend()

# Q3: Pourcentage de retards
plt.subplot(1, 2, 2)
retards = df_delays['delay_at_checkout_in_minutes'] > 0
plt.pie([retards.sum(), (~retards).sum()], 
        labels=['En retard', '√Ä l\'heure'], 
        autopct='%1.1f%%',
        startangle=90)
plt.title('Proportion de retards')
plt.tight_layout()
plt.show()

# Q4: Retards par type de checkin
print("\n" + "="*50)
print("RETARDS PAR TYPE DE CHECKIN")
print("="*50)
retards_par_type = df_delays.groupby('checkin_type').agg({
    'delay_at_checkout_in_minutes': ['mean', 'median', 'count'],
    'rental_id': lambda x: (df_delays.loc[x.index, 'delay_at_checkout_in_minutes'] > 0).sum()
})
print(retards_par_type)

# Q5: Impact sur la location suivante
# Filtrer les cas o√π il y a une location suivante
df_with_next = df_delays[df_delays['time_delta_with_previous_rental_in_minutes'].notna()].copy()

print("\n" + "="*50)
print("IMPACT SUR LA LOCATION SUIVANTE")
print("="*50)
print(f"Nombre de locations avec location suivante : {len(df_with_next)}")

# Cr√©er une colonne "probl√©matique"
df_with_next['is_problematic'] = (
    (df_with_next['delay_at_checkout_in_minutes'] > 0) & 
    (df_with_next['delay_at_checkout_in_minutes'] > df_with_next['time_delta_with_previous_rental_in_minutes'])
)

print(f"Cas probl√©matiques : {df_with_next['is_problematic'].sum()}")
print(f"Pourcentage : {df_with_next['is_problematic'].sum() / len(df_with_next) * 100:.2f}%")

# Q6: Simulation de diff√©rents seuils
seuils = [0, 30, 60, 120, 240, 480]  # 0min, 30min, 1h, 2h, 4h, 8h

print("\n" + "="*50)
print("SIMULATION DES SEUILS")
print("="*50)

for seuil in seuils:
    # Locations qui seraient bloqu√©es
    blocked = df_with_next[df_with_next['time_delta_with_previous_rental_in_minutes'] < seuil]
    
    # Probl√®mes r√©solus
    problems_solved = df_with_next[
        (df_with_next['is_problematic']) & 
        (df_with_next['time_delta_with_previous_rental_in_minutes'] < seuil)
    ]
    
    print(f"\nSeuil : {seuil} minutes ({seuil/60:.1f}h)")
    print(f"  - Locations bloqu√©es : {len(blocked)} ({len(blocked)/len(df_with_next)*100:.1f}%)")
    print(f"  - Probl√®mes r√©solus : {len(problems_solved)}")
1.3 EDA - Donn√©es Pricing (1h)
python# ===== CHARGEMENT =====
df_pricing = pd.read_csv('data/pricing_data.csv')

# ===== EXPLORATION =====
print("=" * 50)
print("DONN√âES PRICING - OVERVIEW")
print("=" * 50)
print(df_pricing.head())
print(df_pricing.info())
print(df_pricing.describe())

# V√©rifier la variable cible
print("\nVariable cible (prix) :")
print(f"Min : {df_pricing['rental_price_per_day'].min()}")
print(f"Max : {df_pricing['rental_price_per_day'].max()}")
print(f"Moyenne : {df_pricing['rental_price_per_day'].mean():.2f}")

# Distribution de la cible
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.hist(df_pricing['rental_price_per_day'], bins=50, edgecolor='black')
plt.xlabel('Prix par jour')
plt.ylabel('Fr√©quence')
plt.title('Distribution des prix')

plt.subplot(1, 2, 2)
plt.boxplot(df_pricing['rental_price_per_day'])
plt.ylabel('Prix par jour')
plt.title('Boxplot des prix')
plt.tight_layout()
plt.show()

# Corr√©lations
print("\nCorr√©lations avec le prix :")
correlations = df_pricing.corr()['rental_price_per_day'].sort_values(ascending=False)
print(correlations)

# Matrice de corr√©lation
plt.figure(figsize=(12, 8))
sns.heatmap(df_pricing.corr(), annot=True, cmap='coolwarm', center=0)
plt.title('Matrice de corr√©lation')
plt.tight_layout()
plt.show()

üé® PHASE 2 : DASHBOARD STREAMLIT (3-4h)
2.1 Structure du dashboard (30 min)
python# dashboard/app.py

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np

# ===== CONFIGURATION PAGE =====
st.set_page_config(
    page_title="GetAround Analysis",
    page_icon="üöó",
    layout="wide"
)

# ===== CHARGEMENT DES DONN√âES =====
@st.cache_data
def load_data():
    df = pd.read_csv('../data/delay_analysis.csv')
    return df

df = load_data()

# ===== SIDEBAR =====
st.sidebar.title("‚öôÔ∏è Param√®tres")
st.sidebar.markdown("---")

# Filtres
checkin_types = st.sidebar.multiselect(
    "Type de checkin",
    options=df['checkin_type'].unique(),
    default=df['checkin_type'].unique()
)

# ===== TITRE PRINCIPAL =====
st.title("üöó GetAround - Analyse des Retards")
st.markdown("**Objectif** : D√©terminer le seuil optimal de d√©lai minimum entre deux locations")
st.markdown("---")
2.2 Section 1 : Vue d'ensemble (45 min)
python# ===== M√âTRIQUES CL√âS =====
col1, col2, col3, col4 = st.columns(4)

with col1:
    total_rentals = len(df)
    st.metric("Total Locations", f"{total_rentals:,}")

with col2:
    late_rentals = (df['delay_at_checkout_in_minutes'] > 0).sum()
    late_pct = late_rentals / total_rentals * 100
    st.metric("Locations en retard", f"{late_rentals:,}", f"{late_pct:.1f}%")

with col3:
    avg_delay = df[df['delay_at_checkout_in_minutes'] > 0]['delay_at_checkout_in_minutes'].mean()
    st.metric("Retard moyen", f"{avg_delay:.0f} min")

with col4:
    df_with_next = df[df['time_delta_with_previous_rental_in_minutes'].notna()]
    consecutive = len(df_with_next)
    st.metric("Locations cons√©cutives", f"{consecutive:,}")

st.markdown("---")

# ===== DISTRIBUTION DES RETARDS =====
st.subheader("üìä Distribution des retards au checkout")

fig = px.histogram(
    df[df['delay_at_checkout_in_minutes'].notna()],
    x='delay_at_checkout_in_minutes',
    nbins=100,
    title="Distribution des retards (toutes locations)",
    labels={'delay_at_checkout_in_minutes': 'Retard (minutes)'}
)
fig.add_vline(x=0, line_dash="dash", line_color="red", annotation_text="√Ä l'heure")
st.plotly_chart(fig, use_container_width=True)

# ===== RETARDS PAR TYPE =====
st.subheader("üì± Retards par type de checkin")

col1, col2 = st.columns(2)

with col1:
    retards_type = df.groupby('checkin_type').agg({
        'delay_at_checkout_in_minutes': ['mean', 'median'],
        'rental_id': 'count'
    }).reset_index()
    retards_type.columns = ['Type', 'Retard Moyen', 'Retard M√©dian', 'Nombre']
    
    fig = px.bar(
        retards_type,
        x='Type',
        y='Retard Moyen',
        title="Retard moyen par type de checkin",
        text='Retard Moyen'
    )
    fig.update_traces(texttemplate='%{text:.1f} min', textposition='outside')
    st.plotly_chart(fig, use_container_width=True)

with col2:
    # Pourcentage de retards par type
    retards_pct = df.groupby('checkin_type').apply(
        lambda x: (x['delay_at_checkout_in_minutes'] > 0).sum() / len(x) * 100
    ).reset_index()
    retards_pct.columns = ['Type', 'Pourcentage']
    
    fig = px.bar(
        retards_pct,
        x='Type',
        y='Pourcentage',
        title="% de locations en retard par type",
        text='Pourcentage',
        color='Pourcentage',
        color_continuous_scale='Reds'
    )
    fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
    st.plotly_chart(fig, use_container_width=True)
2.3 Section 2 : Simulateur de seuils (1h)
pythonst.markdown("---")
st.subheader("üéØ Simulateur de Seuil Minimum")

# ===== S√âLECTION DU SEUIL =====
col1, col2 = st.columns([1, 3])

with col1:
    threshold = st.slider(
        "Seuil minimum (minutes)",
        min_value=0,
        max_value=720,
        value=120,
        step=30,
        help="D√©lai minimum entre deux locations"
    )
    
    st.write(f"**{threshold} minutes** = **{threshold/60:.1f} heures**")
    
    # Scope
    scope = st.radio(
        "P√©rim√®tre d'application",
        options=["Tous les v√©hicules", "Uniquement Connect", "Mobile + Connect"],
        index=0
    )

with col2:
    # Calcul de l'impact
    df_analysis = df[df['time_delta_with_previous_rental_in_minutes'].notna()].copy()
    
    # Appliquer le scope
    if scope == "Uniquement Connect":
        df_analysis = df_analysis[df_analysis['checkin_type'] == 'connect']
    elif scope == "Mobile + Connect":
        df_analysis = df_analysis[df_analysis['checkin_type'].isin(['connect', 'mobile'])]
    
    # Locations qui seraient bloqu√©es
    blocked = df_analysis[df_analysis['time_delta_with_previous_rental_in_minutes'] < threshold]
    
    # Cas probl√©matiques (retard > d√©lai entre locations)
    df_analysis['is_problematic'] = (
        (df_analysis['delay_at_checkout_in_minutes'] > 0) & 
        (df_analysis['delay_at_checkout_in_minutes'] > df_analysis['time_delta_with_previous_rental_in_minutes'])
    )
    
    total_problems = df_analysis['is_problematic'].sum()
    
    # Probl√®mes qui seraient r√©solus
    problems_solved = df_analysis[
        (df_analysis['is_problematic']) & 
        (df_analysis['time_delta_with_previous_rental_in_minutes'] < threshold)
    ]
    
    # Affichage des m√©triques
    col_a, col_b, col_c = st.columns(3)
    
    with col_a:
        st.metric(
            "Locations bloqu√©es",
            f"{len(blocked):,}",
            f"{len(blocked)/len(df_analysis)*100:.1f}%",
            delta_color="inverse"
        )
    
    with col_b:
        st.metric(
            "Probl√®mes r√©solus",
            f"{len(problems_solved):,}",
            f"{len(problems_solved)/total_problems*100:.1f}%" if total_problems > 0 else "0%"
        )
    
    with col_c:
        # Estimation revenue impact (approximatif)
        revenue_impact = len(blocked) / len(df_analysis) * 100
        st.metric(
            "Impact revenu estim√©",
            f"{revenue_impact:.1f}%",
            delta_color="inverse"
        )

# ===== GRAPHIQUE COMPARATIF =====
st.markdown("### Comparaison de diff√©rents seuils")

thresholds_to_test = [0, 30, 60, 120, 180, 240, 360, 480, 720]
results = []

for t in thresholds_to_test:
    blocked_t = df_analysis[df_analysis['time_delta_with_previous_rental_in_minutes'] < t]
    problems_t = df_analysis[
        (df_analysis['is_problematic']) & 
        (df_analysis['time_delta_with_previous_rental_in_minutes'] < t)
    ]
    
    results.append({
        'Seuil (min)': t,
        'Seuil (h)': t/60,
        'Locations bloqu√©es (%)': len(blocked_t)/len(df_analysis)*100,
        'Probl√®mes r√©solus (%)': len(problems_t)/total_problems*100 if total_problems > 0 else 0
    })

df_results = pd.DataFrame(results)

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=df_results['Seuil (h)'],
    y=df_results['Locations bloqu√©es (%)'],
    mode='lines+markers',
    name='Locations bloqu√©es (%)',
    line=dict(color='red', width=3),
    marker=dict(size=10)
))

fig.add_trace(go.Scatter(
    x=df_results['Seuil (h)'],
    y=df_results['Probl√®mes r√©solus (%)'],
    mode='lines+markers',
    name='Probl√®mes r√©solus (%)',
    line=dict(color='green', width=3),
    marker=dict(size=10)
))

fig.update_layout(
    title="Trade-off : Locations bloqu√©es vs Probl√®mes r√©solus",
    xaxis_title="Seuil (heures)",
    yaxis_title="Pourcentage (%)",
    hovermode='x unified',
    height=500
)

st.plotly_chart(fig, use_container_width=True)

# Tableau des r√©sultats
st.dataframe(df_results.style.format({
    'Seuil (h)': '{:.1f}h',
    'Locations bloqu√©es (%)': '{:.1f}%',
    'Probl√®mes r√©solus (%)': '{:.1f}%'
}), use_container_width=True)
2.4 Section 3 : Insights & Recommandations (30 min)
pythonst.markdown("---")
st.subheader("üí° Insights Cl√©s")

col1, col2 = st.columns(2)

with col1:
    st.markdown("#### üîç Observations")
    st.markdown(f"""
    - **{late_pct:.1f}%** des locations sont en retard
    - Le retard moyen est de **{avg_delay:.0f} minutes**
    - **{total_problems:,}** cas probl√©matiques identifi√©s
    - Les locations **Connect** ont {df[df['checkin_type']=='connect']['delay_at_checkout_in_minutes'].mean():.0f} min de retard en moyenne
    """)

with col2:
    st.markdown("#### üéØ Recommandations")
    
    # Calcul du seuil optimal (exemple)
    optimal_threshold = 120  # √Ä ajuster selon analyse
    st.markdown(f"""
    - **Seuil recommand√©** : {optimal_threshold} minutes ({optimal_threshold/60:.0f}h)
    - **P√©rim√®tre** : Commencer avec Connect uniquement
    - **B√©n√©fice attendu** : R√©solution de ~70% des probl√®mes
    - **Impact revenu** : < 10% des locations affect√©es
    """)

# ===== DONN√âES BRUTES =====
with st.expander("üìã Voir les donn√©es brutes"):
    st.dataframe(df.head(100), use_container_width=True)
2.5 Test local (15 min)
bash# Dans le terminal
cd dashboard
streamlit run app.py

ü§ñ PHASE 3 : MACHINE LEARNING (4-5h)
3.1 Pr√©paration des donn√©es (1h)
python# notebooks/02_ML_pricing.ipynb

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib

# ===== CHARGEMENT =====
df = pd.read_csv('../data/pricing_data.csv')

print("=" * 50)
print("PR√âPARATION DES DONN√âES ML")
print("=" * 50)

# ===== NETTOYAGE =====
print("\n1. Valeurs manquantes :")
print(df.isnull().sum())

# Supprimer les lignes avec valeurs manquantes (ou imputer selon le cas)
df_clean = df.dropna()

print(f"\nLignes apr√®s nettoyage : {len(df_clean)}")

# ===== S√âPARATION X et y =====
# Identifier la colonne cible (g√©n√©ralement 'rental_price_per_day')
target = 'rental_price_per_day'

X = df_clean.drop(target, axis=1)
y = df_clean[target]

print(f"\n2. Features : {X.shape[1]}")
print(f"Colonnes : {X.columns.tolist()}")

# ===== SPLIT TRAIN/TEST =====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\n3. Split des donn√©es :")
print(f"Train : {len(X_train)} lignes")
print(f"Test  : {len(X_test)} lignes")

# ===== STANDARDISATION =====
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n4. Standardisation effectu√©e ‚úì")
3.2 Entra√Ænement des mod√®les (2h)
python# ===== MOD√àLES √Ä TESTER =====
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)
}

results = {}

print("\n" + "=" * 50)
print("ENTRA√éNEMENT DES MOD√àLES")
print("=" * 50)

for name, model in models.items():
    print(f"\nüîÑ Entra√Ænement : {name}...")
    
    # Entra√Ænement
    model.fit(X_train_scaled, y_train)
    
    # Pr√©dictions
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)
    
    # M√©triques
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)
    
    results[name] = {
        'model': model,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'test_mae': test_mae
    }
    
    print(f"  ‚úì R¬≤ Train : {train_r2:.4f}")
    print(f"  ‚úì R¬≤ Test  : {test_r2:.4f}")
    print(f"  ‚úì RMSE     : {test_rmse:.2f}")
    print(f"  ‚úì MAE      : {test_mae:.2f}")

# ===== S√âLECTION DU MEILLEUR MOD√àLE =====
best_model_name = max(results, key=lambda x: results[x]['test_r2'])
best_model = results[best_model_name]['model']

print("\n" + "=" * 50)
print(f"üèÜ MEILLEUR MOD√àLE : {best_model_name}")
print("=" * 50)
print(f"R¬≤ Test : {results[best_model_name]['test_r2']:.4f}")
print(f"RMSE    : {results[best_model_name]['test_rmse']:.2f}")

# ===== IMPORTANCE DES FEATURES (si Random Forest) =====
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüìä Top 10 Features importantes :")
    print(feature_importance.head(10))
    
    # Visualisation
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'].head(10), feature_importance['importance'].head(10))
    plt.xlabel('Importance')
    plt.title('Top 10 Features')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
3.3 Optimisation (optionnel) (1h)
pythonfrom sklearn.model_selection import GridSearchCV

# ===== GRID SEARCH =====
print("\nüîç Optimisation des hyperparam√®tres...")

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)

print(f"\n‚úì Meilleurs param√®tres : {grid_search.best_params_}")
print(f"‚úì Meilleur score CV : {grid_search.best_score_:.4f}")

# Utiliser le meilleur mod√®le
best_model_optimized = grid_search.best_estimator_

# √âvaluation
y_pred_optimized = best_model_optimized.predict(X_test_scaled)
r2_optimized = r2_score(y_test, y_pred_optimized)

print(f"‚úì R¬≤ Test apr√®s optimisation : {r2_optimized:.4f}")
3.4 Sauvegarde du mod√®le (15 min)
python# ===== SAUVEGARDE =====
import joblib

# Cr√©er un dictionnaire avec tout ce qui est n√©cessaire
model_package = {
    'model': best_model_optimized,
    'scaler': scaler,
    'feature_names': X.columns.tolist(),
    'metrics': {
        'r2': r2_optimized,
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_optimized))
    }
}

# Sauvegarder
joblib.dump(model_package, '../api/model.pkl')

print("\n‚úÖ Mod√®le sauvegard√© dans api/model.pkl")

# ===== TEST DE RECHARGEMENT =====
loaded_package = joblib.load('../api/model.pkl')
print("\n‚úì Test de rechargement r√©ussi")
print(f"Features : {loaded_package['feature_names']}")
print(f"R¬≤ : {loaded_package['metrics']['r2']:.4f}")

üöÄ PHASE 4 : API FASTAPI (3-4h)
4.1 Structure de l'API (1h)
python# api/main.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import joblib
import numpy as np
import pandas as pd

# ===== INITIALISATION =====
app = FastAPI(
    title="GetAround Pricing API",
    description="API de pr√©diction des prix de location de voitures",
    version="1.0.0"
)

# ===== CHARGEMENT DU MOD√àLE =====
try:
    model_package = joblib.load('model.pkl')
    model = model_package['model']
    scaler = model_package['scaler']
    feature_names = model_package['feature_names']
    print("‚úÖ Mod√®le charg√© avec succ√®s")
except Exception as e:
    print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
    model = None

# ===== MOD√àLES PYDANTIC =====
class PredictionInput(BaseModel):
    input: List[List[float]]
    
    class Config:
        json_schema_extra = {
            "example": {
                "input": [
                    [7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8],
                    [6.5, 0.30, 0.40, 21.0, 0.050, 50.0, 180.0, 1.002, 3.1, 0.50, 9.0]
                ]
            }
        }

class PredictionOutput(BaseModel):
    prediction: List[float]

# ===== ENDPOINTS =====

@app.get("/")
def read_root():
    """
    Page d'accueil de l'API
    """
    return {
        "message": "Bienvenue sur l'API GetAround Pricing",
        "endpoints": {
            "/predict": "POST - Pr√©diction des prix",
            "/docs": "Documentation interactive",
            "/health": "Statut de l'API"
        }
    }

@app.get("/health")
def health_check():
    """
    V√©rifie que l'API fonctionne correctement
    """
    if model is None:
        raise HTTPException(status_code=500, detail="Mod√®le non charg√©")
    
    return {
        "status": "healthy",
        "model_loaded": True,
        "features_count": len(feature_names)
    }

@app.post("/predict", response_model=PredictionOutput)
def predict(data: PredictionInput):
    """
    Effectue des pr√©dictions de prix
    
    - **input**: Liste de listes de features (une liste par voiture √† pr√©dire)
    
    Retourne une liste de prix pr√©dits
    """
    try:
        # Convertir en numpy array
        X = np.array(data.input)
        
        # V√©rifier la shape
        if X.shape[1] != len(feature_names):
            raise HTTPException(
                status_code=400,
                detail=f"Nombre de features incorrect. Attendu: {len(feature_names)}, Re√ßu: {X.shape[1]}"
            )
        
        # Standardiser
        X_scaled = scaler.transform(X)
        
        # Pr√©dire
        predictions = model.predict(X_scaled)
        
        # Arrondir √† 2 d√©cimales
        predictions = [round(float(p), 2) for p in predictions]
        
        return PredictionOutput(prediction=predictions)
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de la pr√©diction: {str(e)}")

@app.get("/features")
def get_features():
    """
    Retourne la liste des features attendues par le mod√®le
    """
    return {
        "features": feature_names,
        "count": len(feature_names)
    }
4.2 Test local de l'API (30 min)
bash# Dans le terminal
cd api
uvicorn main:app --reload --host 0.0.0.0 --port 8000
python# test_api.py
import requests
import json

# URL de l'API locale
url = "http://localhost:8000/predict"

# Donn√©es de test
data = {
    "input": [
        [7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8],
        [6.5, 0.30, 0.40, 21.0, 0.050, 50.0, 180.0, 1.002, 3.1, 0.50, 9.0]
    ]
}

# Requ√™te POST
response = requests.post(url, json=data)

print(f"Status Code: {response.status_code}")
print(f"Response: {response.json()}")

# Test avec curl
"""
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"input": [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]}'
"""
4.3 Documentation HTML (1h)
python# api/templates/docs.html (si tu veux une page custom)
# OU utilise la doc auto de FastAPI √† /docs

# Ajouter un endpoint pour une doc custom
from fastapi.responses import HTMLResponse

@app.get("/documentation", response_class=HTMLResponse)
def documentation():
    """
    Documentation HTML personnalis√©e
    """
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>GetAround API - Documentation</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f5f5f5;
            }
            h1 { color: #2c3e50; }
            h2 { color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 10px; }
            .endpoint {
                background: white;
                padding: 20px;
                margin: 20px 0;
                border-radius: 5px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            }
            .method {
                display: inline-block;
                padding: 5px 10px;
                border-radius: 3px;
                font-weight: bold;
                color: white;
            }
            .post { background-color: #49cc90; }
            .get { background-color: #61affe; }
            code {
                background-color: #f4f4f4;
                padding: 2px 5px;
                border-radius: 3px;
                font-family: 'Courier New', monospace;
            }
            pre {
                background-color: #f4f4f4;
                padding: 15px;
                border-radius: 5px;
                overflow-x: auto;
            }
        </style>
    </head>
    <body>
        <h1>üöó GetAround Pricing API</h1>
        <p>API de pr√©diction des prix de location de voitures utilisant le Machine Learning</p>
        
        <div class="endpoint">
            <h2><span class="method post">POST</span> /predict</h2>
            <p><strong>Description:</strong> Effectue des pr√©dictions de prix pour une ou plusieurs voitures</p>
            
            <h3>Input (JSON)</h3>
            <pre><code>{
  "input": [
    [7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8],
    [6.5, 0.30, 0.40, 21.0, 0.050, 50.0, 180.0, 1.002, 3.1, 0.50, 9.0]
  ]
}</code></pre>

            <h3>Output (JSON)</h3>
            <pre><code>{
  "prediction": [125.50, 132.75]
}</code></pre>

            <h3>Exemple avec cURL</h3>
            <pre><code>curl -X POST "https://your-api.com/predict" \\
  -H "Content-Type: application/json" \\
  -d '{"input": [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]}'</code></pre>

            <h3>Exemple avec Python</h3>
            <pre><code>import requests

response = requests.post(
    "https://your-api.com/predict",
    json={"input": [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]}
)
print(response.json())</code></pre>
        </div>

        <div class="endpoint">
            <h2><span class="method get">GET</span> /health</h2>
            <p><strong>Description:</strong> V√©rifie le statut de l'API</p>
            
            <h3>Output (JSON)</h3>
            <pre><code>{
  "status": "healthy",
  "model_loaded": true,
  "features_count": 11
}</code></pre>
        </div>

        <div class="endpoint">
            <h2><span class="method get">GET</span> /features</h2>
            <p><strong>Description:</strong> Retourne la liste des features attendues</p>
            
            <h3>Output (JSON)</h3>
            <pre><code>{
  "features": ["feature1", "feature2", "..."],
  "count": 11
}</code></pre>
        </div>

        <footer style="margin-top: 50px; text-align: center; color: #7f8c8d;">
            <p>Documentation interactive compl√®te disponible sur <a href="/docs">/docs</a></p>
        </footer>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

üåê PHASE 5 : D√âPLOIEMENT (2-3h)
5.1 Pr√©paration Hugging Face (1h)
python# api/requirements.txt
fastapi==0.101.0
uvicorn==0.23.2
pydantic==2.1.1
joblib==1.3.2
scikit-learn==1.3.0
numpy==1.24.3
pandas==2.0.3

# api/app.py (renommer main.py en app.py pour HF)
# [m√™me contenu que main.py]

# Cr√©er un fichier README.md pour Hugging Face
5.2 D√©ploiement sur Hugging Face Spaces (1h)
markdown# README_HF.md

---
title: GetAround Pricing API
emoji: üöó
colorFrom: blue
colorTo: green
sdk: docker
pinned: false
---

# GetAround Pricing API

API de pr√©diction des prix de location de voitures

## Endpoints

- `POST /predict` - Pr√©diction de prix
- `GET /docs` - Documentation interactive
- `GET /health` - Statut de l'API
dockerfile# Dockerfile pour Hugging Face
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]
√âtapes sur Hugging Face:

Cr√©er un compte sur huggingface.co
New Space ‚Üí Docker
Upload tes fichiers (app.py, model.pkl, requirements.txt, Dockerfile)
Space se d√©ploie automatiquement

5.3 D√©ploiement Dashboard Streamlit (30 min)
bash# dashboard/requirements.txt
streamlit==1.25.0
pandas==2.0.3
plotly==5.15.0
numpy==1.24.3
√âtapes sur Streamlit Cloud:

Push ton code sur GitHub
streamlit.io/cloud ‚Üí New app
Connecte ton repo GitHub
S√©lectionne dashboard/app.py
Deploy!